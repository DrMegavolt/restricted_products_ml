{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Approach  ~0.94\n",
    "1. Join all columns that have text\n",
    "1. Clean html tags\n",
    "1. Replace non english letters with equivalent Å -> A\n",
    "1. lowercase\n",
    "1. Google BERT Base model with 256 tokens and fine tuned 2 layers + Dense 256 layer on top\n",
    "\n",
    "#### More thorrow cleunup + BERT Custom dictionary ~0.98\n",
    "1. Remove weight references like 600g\n",
    "1. Remove size references (150cm x 16cm) but keep words like length, diameter etc.\n",
    "1. Fix incorrect labels in train dataset (I only found 1, but there are more)\n",
    "1. Read \"Start & Run an Adult Boutique\" book and write all adult toys related words into bert Dictionary file \n",
    "https://books.google.ca/books?id=LMQhCgAAQBAJ&pg=PT157&lpg=PT157&dq=adult+toys+vocabulary&source=bl&ots=OneH8Cmfa2&sig=ACfU3U3_8mllbStNXxpfJ6WLhrTK3IdiFQ&hl=en&sa=X&redir_esc=y#v=onepage&q=adult%20toys%20vocabulary&f=false\n",
    "\n",
    "\n",
    "####  ~0.99+\n",
    "\n",
    "1. Add some common intentional misspellings like \"didlo\"\n",
    "1. Add training samples based on those words with label 1 to force BERT pay higher attention\n",
    "1. Count Å -> A replacements and add special custom word \"nel\" if there are too many of those. Usually this means people are trying to cheat and hide real word. (not sure if it helps)\n",
    "1. Train 3 top BERT layers instead of 2 (although usually does not converge, some luck required)\n",
    "1. Increase number of token from 256 to 512 to catch more words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import bertkeras as bk\n",
    "import BERTDictionary as bd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "from unidecode import unidecode\n",
    "from scipy.stats import norm, boxcox\n",
    "pd.options.display.max_colwidth = 1000\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('../dataset/training.csv', index_col='ID')\n",
    "labels = df['label']\n",
    "drop_columns = [ '﻿ASIN','parent_asin','Product Group Description', \n",
    "                      'product_name', 'brand_code', 'brand_name']\n",
    "df = df.drop(columns=drop_columns)\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "cols = df.columns.tolist()\n",
    "cols.remove('label')\n",
    "text = df[cols].apply(' '.join, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_re = re.compile('<[^>]*>')\n",
    "garbage_re = re.compile('[^0-9a-z,\\'\\.]+') \n",
    "weight_re = re.compile('[\\d.]+[ ]?(kg+|lbs+|g+|lb+)')\n",
    "\n",
    "\n",
    "# check if it actually helps, tests https://regex101.com/r/xwnV5l/1\n",
    "size_re = re.compile('[\\d.]+([ cminch]+)?[ x*]+[\\d.]+([ cminch]+)?([ x*]+[\\d.]+)?([ cminch]+)')\n",
    "\n",
    "# play around with garbage_threshhold \n",
    "garbage_threshhold = 0.003\n",
    "def html_cleanup(html):\n",
    "    return re.sub(html_re, ' ', html)\n",
    "def lower_case(txt):\n",
    "    return txt.lower()\n",
    "def split_by_garbage(txt):\n",
    "    return re.sub(garbage_re, ' ', txt)\n",
    "def decode_chars(txt):\n",
    "    garbage_fiteted = re.sub(garbage_re, '', txt)\n",
    "    decoded_txt = unidecode(txt)\n",
    "    return decoded_txt if len(garbage_fiteted)/len(txt) < garbage_threshhold \\\n",
    "        else '[NEL] ' + decoded_txt \n",
    "\n",
    "def cleanup_size(txt):\n",
    "    return re.sub(size_re, ' ', txt).replace('size:', '')\n",
    "\n",
    "def cleanup_weight(txt):\n",
    "    return re.sub(weight_re, ' ', txt)\n",
    "    \n",
    "def cleanup(text):\n",
    "    res = text.apply(html_cleanup).apply(decode_chars).apply(lower_case) \\\n",
    "              .apply(split_by_garbage).apply(cleanup_size) \\\n",
    "              .apply(cleanup_weight)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = cleanup(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame( {\"texts\" :texts, \"labels\": labels} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is error in training dataset\n",
    "df.loc[196,'labels'] =1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_train, txt_test, lbl_train, lbl_test = train_test_split(\n",
    "    texts, labels, test_size=0.0002, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[NEL]' 'nel' 'porn' 'nubby']\n",
      "['lesbian' 'gay' 'transvestite' 'transgender']\n",
      "['lube' 'lubricant' 'sex' 'intercourse']\n",
      "['fuck' 'vibrate' 'suction' 'lick']\n",
      "['suck' 'gag' 'dildo' 'vibrator']\n",
      "['condom' 'anal' 'plug' 'snake']\n",
      "['trembler' 'pulsator' 'lockaid' 'strap']\n",
      "['buzzer' 'rabbit' 'egg' 'penetrator']\n",
      "['dual' 'kegel' 'ben']\n",
      "['wa' 'stimulator' 'masturbator']\n",
      "['lasso' 'bondage' 'cuff']\n",
      "['pasties' 'strapon' 'didlo']\n",
      "['dido' 'dilo' 'dlo']\n",
      "['device' 'gadget' 'pleasure']\n",
      "['intimate' 'last' 'aroused']\n",
      "['climax' 'enjoyment' 'orgasm']\n",
      "['bdsm' 'sadism' 'submission']\n",
      "['masochism' 'realistic' 'stimulate']\n",
      "['squirt' 'virginity' 'dick']\n",
      "['cock' 'penis' 'spot']\n",
      "['butt' 'ass' 'nipple']\n",
      "['clitor' 'pelvic' 'vagina']\n",
      "['pussy' 'anus' 'prostate']\n",
      "['rectum' 'ventral' 'scrotum']\n",
      "['tongue' 'perineum' 'boob']\n"
     ]
    }
   ],
   "source": [
    "k=10000\n",
    "arr = np.array_split(bd.adult_toys_vocab, 25)\n",
    "for a in arr :\n",
    "    print(a)\n",
    "    w = ' '.join(a)\n",
    "    txt_train = txt_train.append(pd.Series([w], index=[k]))\n",
    "    lbl_train = lbl_train.append(pd.Series([1], index=[k]))\n",
    "    k = k+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using vocab file from vocab.txt\n"
     ]
    }
   ],
   "source": [
    "tokenizer = bk.create_tokenizer_from_hub_module(vocab_file='vocab.txt')\n",
    "max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to InputExample format\n",
    "train_examples = bk.convert_text_to_examples(txt_train, lbl_train.values)\n",
    "test_examples = bk.convert_text_to_examples(txt_test, lbl_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753900d61d62421daadc830cc8d04c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=5509, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e0a3e7760d4059a7c6439090942f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=2, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert to features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",
    ") = bk.convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels\n",
    ") = bk.convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': None,\n",
       " 'text_a': \" nel lqqbstorage celtic decor,grommet window curtain,head of legend dragon with ethnic african ornate effects on grunge backdrop myth celtic design, noise reducing curtain,multi multifunction 1.curtains is delicately designed, allowing you to decorate your windows with great styles,protect your furniture, floors and walls from the sun. 2.featuring vibrant colored pattern, this eye soothing curtain can match interiors of any room. 3.soft and durable window curtains has a very soft hand feel and drapery, high quality material, durable 10 years. 4.privacy and relax your family and friends can enjoy movie nights or sports game without worrying about outside light and noise. 5.energy efficient innovative triple weave construction insulate your sunny or frozen window, give your air conditioner and energy bill a break. 6.reduce noise densely woven fabric acts as an additional sound barrier,it reduces outside noises and create a quiet and peaceful environment. widely use for perfect for window in living room, bedroom, dining room, bathroom and kitchen. color may differ slightly from photos due to different monitor settings in computer monitors display or mobile phone, filming angles, lights, sunshine ect. product sizes that measured by hand lead to the difference between about  due to everyone's measurement technique. after sale guarantee we offer 100 risk free and satisfactory after sales service. if you have any problem, support 30 days refund without reason hot sale size w63 x l ,measurements are the total of the 2 curtain panels together, the size of each panel is w31.5 x l72 ,each panel has 8 elegant metal grommets, inner diameter of grommet is 1.6 , fit well with standard or decorative curtain rods polyester material 100 polyester. material soft and smooth fabric, sturdy hemming, durable and long lasting, no loose threads. we have 10 sizes and thousands of colors for you to chose,your own picture of customization is also acceptable. meet various needs and purposes of different people. multiple functions these soft and durable curtains are made by innovative technology durable yarn, high performance on room darkening, thermal insulated, noise reducing, energy saving efficiency and privacy protection unique home decor these customized curtain panels will decorate your windows and rooms with new great effect, suits for contemporary, traditional, vintage rustic, victorian home styles. let these breatheable and airy blackout curtains add amazing look to host and guest best service 10 years quality warranty and 30 days money back guarantee for return policy. machine washable for easy care, use only non chlorine bleach when needed, tumble dry low and warm iron as needed. \",\n",
       " 'text_b': None,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_seq_length): \n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    def auc(y_true, y_pred):\n",
    "        auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "#     K.get_session().run(tf.local_variables_initializer())\n",
    "        return auc\n",
    "    bert_output = bk.BertLayer(n_fine_tune_layers=2, pooling=\"first\")(bert_inputs)\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC_THRESHOLD = 0.99\n",
    "LOSS_THRESHOLD = 0.015\n",
    "class myCallback(tf.keras.callbacks.Callback): \n",
    "    def on_batch_end(self, batch, logs): \n",
    "        if(logs.get('auc') > METRIC_THRESHOLD and logs.get('loss')<LOSS_THRESHOLD):   \n",
    "          print(\"\\nReached %2.2f%% auc, so stopping training!!\" %(METRIC_THRESHOLD*100)) \n",
    "          self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_2 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          196864      bert_layer_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            257         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 14,963,457\n",
      "Non-trainable params: 95,338,554\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5509 samples, validate on 2 samples\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[12288,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bert_layer_2/bert_layer_2_module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/moments/SquaredDifference}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node metrics_2/auc/div_no_nan}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-e1e5e266740d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop98\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;32m~/miniconda3/envs/restricted_products/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/restricted_products/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/restricted_products/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/miniconda3/envs/restricted_products/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/restricted_products/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[12288,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bert_layer_2/bert_layer_2_module_apply_tokens/bert/encoder/layer_10/output/LayerNorm/moments/SquaredDifference}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node metrics_2/auc/div_no_nan}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "# Initialize session\n",
    "batch_size = 24\n",
    "sess = tf.Session()\n",
    "# Instantiate variables\n",
    "bk.initialize_vars(sess)\n",
    "tb = tf.compat.v1.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=batch_size, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='batch')\n",
    "stop98 = myCallback()\n",
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n",
    "    callbacks = [ tb, stop98], #, \n",
    "    epochs=8,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('restricted.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subm= pd.read_csv('../dataset/public_test_features.csv', index_col='ID')\n",
    "df_subm = df_subm.drop(columns=drop_columns)\n",
    "df_subm = df_subm.fillna(\"\")\n",
    "\n",
    "text_subm = df_subm[cols].apply(' '.join, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_subm = cleanup(text_subm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_examples = bk.convert_text_to_examples(texts_subm, [0]*len(texts_subm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(subm_input_ids, subm_input_masks, subm_segment_ids, subm_labels\n",
    ") = bk.convert_examples_to_features(tokenizer, subm_examples, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm_examples[0].text_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testCount = len(subm_input_ids)\n",
    "def pickMax(arr):\n",
    "  m = max(arr)\n",
    "  return [x for x in arr]\n",
    "\n",
    "threshold = 0.4\n",
    "def tolabel(arr):\n",
    "  # return int(max(arr)>=threshold)\n",
    "  return max(arr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([subm_input_ids[0:testCount], \n",
    "                                subm_input_masks[0:testCount], \n",
    "                                subm_segment_ids[0:testCount]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [ tolabel(p) for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"text\":texts_subm, \"label\":pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results['label']>0.03][results['label']<0.95]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('results.csv', columns=[ 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
